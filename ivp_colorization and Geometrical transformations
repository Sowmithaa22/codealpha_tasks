{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10360011,"sourceType":"datasetVersion","datasetId":6416044},{"sourceId":10360319,"sourceType":"datasetVersion","datasetId":6416282},{"sourceId":10360547,"sourceType":"datasetVersion","datasetId":6416466}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sowmithaasris/ivp-color-trns-ipynb?scriptVersionId=215881397\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage import color\nfrom PIL import Image\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T04:50:50.838816Z","iopub.execute_input":"2025-01-03T04:50:50.839187Z","iopub.status.idle":"2025-01-03T04:50:50.843779Z","shell.execute_reply.started":"2025-01-03T04:50:50.839156Z","shell.execute_reply":"2025-01-03T04:50:50.842465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Image\nimage= cv2.imread('/kaggle/input/puppyy/2.jpg')\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.subplot(1,1,1)\nplt.title('Original')\nplt.imshow(image_rgb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T04:50:13.42017Z","iopub.execute_input":"2025-01-03T04:50:13.420525Z","iopub.status.idle":"2025-01-03T04:50:13.864505Z","shell.execute_reply.started":"2025-01-03T04:50:13.420495Z","shell.execute_reply":"2025-01-03T04:50:13.863378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)\nimage_hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)\nimage_lab = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2LAB)\nimage_ycbcr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2YCrCb)\n\nimages = [image_gray, image_hsv[..., 0], image_lab[..., 0], image_ycbcr[..., 0]]\ntitles = ['Grayscale', 'HSV (Hue)', 'LAB (Lightness)', 'YCbCr (Luma)']\ncmaps = ['gray', 'hsv', 'gray', 'gray']\n\n# Plot the images in a 2x2 grid\nfig, axs = plt.subplots(2, 2, figsize=(8, 8))\n\n# Loop to display each image\nfor ax, img, title, cmap in zip(axs.ravel(), images, titles, cmaps):\n    ax.imshow(img, cmap=cmap)\n    ax.set_title(title)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T04:50:53.651423Z","iopub.execute_input":"2025-01-03T04:50:53.651809Z","iopub.status.idle":"2025-01-03T04:50:54.296681Z","shell.execute_reply.started":"2025-01-03T04:50:53.651777Z","shell.execute_reply":"2025-01-03T04:50:54.295641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = '/kaggle/input/puppyy/2.jpg'  \nimage_rgb = Image.open(image_path).convert(\"RGB\")\nimage_cmyk = image_rgb.convert(\"CMYK\")\n# Convert to numpy array for displaying each channel\nimage_cmyk_array = np.array(image_cmyk)\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\naxs[0].imshow(image_cmyk_array[..., 0], cmap='Blues')\naxs[0].set_title('CMYK (Cyan)')\naxs[0].axis('off')\naxs[1].imshow(image_cmyk_array[..., 1], cmap='Purples')\naxs[1].set_title('CMYK (Magenta)')\naxs[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T04:54:32.274087Z","iopub.execute_input":"2025-01-03T04:54:32.274519Z","iopub.status.idle":"2025-01-03T04:54:32.749033Z","shell.execute_reply.started":"2025-01-03T04:54:32.274487Z","shell.execute_reply":"2025-01-03T04:54:32.747928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_array = np.array(image_rgb)\nred_channel = image_array[..., 0]\ngreen_channel = image_array[..., 1]\nblue_channel = image_array[..., 2]\n\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\naxs[0].imshow(red_channel, cmap='Reds')\naxs[0].set_title('Red Channel')\naxs[0].axis('off')\naxs[1].imshow(green_channel, cmap='Greens')\naxs[1].set_title('Green Channel')\naxs[1].axis('off')\naxs[2].imshow(blue_channel, cmap='Blues')\naxs[2].set_title('Blue Channel')\naxs[2].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:00:08.883974Z","iopub.execute_input":"2025-01-03T05:00:08.884373Z","iopub.status.idle":"2025-01-03T05:00:09.494818Z","shell.execute_reply.started":"2025-01-03T05:00:08.884333Z","shell.execute_reply":"2025-01-03T05:00:09.49373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Increase the intensity of the Red channel by 30%\nred_channel = np.clip(red_channel * 1.3, 0, 255).astype(np.uint8)\n\n# Combine the modified Red channel with the original Green and Blue channels\nimage_modified = np.dstack((red_channel, green_channel, blue_channel))\n\n# Convert the modified image array back to a Pillow image\nimage_modified_pil = Image.fromarray(image_modified)\n\n# Plot the original and modified images\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\naxs[0].imshow(image_rgb)\naxs[0].set_title('Original Image')\naxs[0].axis('off')\naxs[1].imshow(image_modified_pil)\naxs[1].set_title('Modified Image (Red +30%)')\naxs[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:06:06.702679Z","iopub.execute_input":"2025-01-03T05:06:06.703034Z","iopub.status.idle":"2025-01-03T05:06:07.308389Z","shell.execute_reply.started":"2025-01-03T05:06:06.703006Z","shell.execute_reply":"2025-01-03T05:06:07.307273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_gray = image_rgb.convert(\"L\")\nimage_gray_array = np.array(image_gray)\nimage_pseudo_colored = plt.cm.jet(image_gray_array / 255.0)  # Normalize to [0, 1] range\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\naxs[0].imshow(image_gray, cmap='gray')\naxs[0].set_title('Original Grayscale')\naxs[0].axis('off')\n\naxs[1].imshow(image_pseudo_colored)\naxs[1].set_title('Pseudo-colored (Jet Colormap)')\naxs[1].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:09:39.058272Z","iopub.execute_input":"2025-01-03T05:09:39.05863Z","iopub.status.idle":"2025-01-03T05:09:39.662462Z","shell.execute_reply.started":"2025-01-03T05:09:39.058601Z","shell.execute_reply":"2025-01-03T05:09:39.661151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_inverted_array = 255 - image_array\n\n# Convert the inverted numpy array back to a Pillow image\nimage_inverted = Image.fromarray(image_inverted_array)\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\naxs[0].imshow(image_rgb)\naxs[0].set_title('Original Image')\naxs[0].axis('off')\naxs[1].imshow(image_inverted)\naxs[1].set_title('Color Inverted Image')\naxs[1].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:13:11.473456Z","iopub.execute_input":"2025-01-03T05:13:11.473842Z","iopub.status.idle":"2025-01-03T05:13:12.064194Z","shell.execute_reply.started":"2025-01-03T05:13:11.473812Z","shell.execute_reply":"2025-01-03T05:13:12.062968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scale the image to half its original size using Bilinear interpolation\nimage_half_bilinear = image_rgb.resize(\n    (image_rgb.width // 2, image_rgb.height // 2), Image.BILINEAR)\n\n# Scale the image to half its original size using Nearest-Neighbor interpolation\nimage_half_nearest = image_rgb.resize(\n    (image_rgb.width // 2, image_rgb.height // 2), Image.NEAREST)\n\n# Scale the image back to double its original size using Bilinear interpolation\nimage_double_bilinear = image_half_bilinear.resize(\n    (image_rgb.width, image_rgb.height), Image.BILINEAR)\n\n# Scale the image back to double its original size using Nearest-Neighbor interpolation\nimage_double_nearest = image_half_nearest.resize(\n    (image_rgb.width, image_rgb.height), Image.NEAREST)\n\nfig, axs = plt.subplots(2, 3, figsize=(15, 10))\naxs[0, 0].imshow(image_rgb)\naxs[0, 0].set_title('Original Image')\naxs[0, 0].axis('off')\naxs[0, 1].imshow(image_half_bilinear)\naxs[0, 1].set_title('Half Size (Bilinear)')\naxs[0, 1].axis('off')\naxs[0, 2].imshow(image_half_nearest)\naxs[0, 2].set_title('Half Size (Nearest-Neighbor)')\naxs[0, 2].axis('off')\naxs[1, 0].imshow(image_double_bilinear)\naxs[1, 0].set_title('Double Size (Bilinear)')\naxs[1, 0].axis('off')\naxs[1, 1].imshow(image_double_nearest)\naxs[1, 1].set_title('Double Size (Nearest-Neighbor)')\naxs[1, 1].axis('off')\naxs[1, 2].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:22:31.469849Z","iopub.execute_input":"2025-01-03T05:22:31.470278Z","iopub.status.idle":"2025-01-03T05:22:32.767692Z","shell.execute_reply.started":"2025-01-03T05:22:31.470237Z","shell.execute_reply":"2025-01-03T05:22:32.766336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_translation = 150 \ny_translation = 130  \n# Create an output image with the same size, filled with black (0) pixels\ntranslated_image = np.zeros_like(image_array)\n\ntranslated_image[y_translation:, x_translation:] = image_array[:-y_translation, :-x_translation]\n\n# Convert the translated image array back to a Pillow image\ntranslated_image_pil = Image.fromarray(translated_image)\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\naxs[0].imshow(image_rgb)\naxs[0].set_title('Original Image')\naxs[0].axis('off')\naxs[1].imshow(translated_image_pil)\naxs[1].set_title('Translated Image (150px right, 130px down)')\naxs[1].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:30:27.151317Z","iopub.execute_input":"2025-01-03T05:30:27.151686Z","iopub.status.idle":"2025-01-03T05:30:27.72926Z","shell.execute_reply.started":"2025-01-03T05:30:27.151658Z","shell.execute_reply":"2025-01-03T05:30:27.728142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_rot_45_clip = image_rgb.rotate(45)  \nimage_rot_90_clip = image_rgb.rotate(90)  \nimage_rot_180_clip = image_rgb.rotate(180)  \n\n# Without clipping (expand the image to avoid clipping edges)\nimage_rot_45_no_clip = image_rgb.rotate(45, expand=True)  \nimage_rot_90_no_clip = image_rgb.rotate(90, expand=True)  \nimage_rot_180_no_clip = image_rgb.rotate(180, expand=True)\n\nfig, axs = plt.subplots(3, 4, figsize=(15, 12))\naxs[0, 1].imshow(image_rot_45_clip)\naxs[0, 1].set_title('45° Rotation (Clipping)')\naxs[0, 1].axis('off')\n\naxs[0, 2].imshow(image_rot_90_clip)\naxs[0, 2].set_title('90° Rotation (Clipping)')\naxs[0, 2].axis('off')\n\naxs[0, 3].imshow(image_rot_180_clip)\naxs[0, 3].set_title('180° Rotation (Clipping)')\naxs[0, 3].axis('off')\n\naxs[1, 0].imshow(image_rot_45_no_clip)\naxs[1, 0].set_title('45° Rotation (No Clipping)')\naxs[1, 0].axis('off')\n\naxs[1, 1].imshow(image_rot_90_no_clip)\naxs[1, 1].set_title('90° Rotation (No Clipping)')\naxs[1, 1].axis('off')\n\naxs[1, 2].imshow(image_rot_180_no_clip)\naxs[1, 2].set_title('180° Rotation (No Clipping)')\naxs[1, 2].axis('off')\n#axs[1, 3].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:34:57.692402Z","iopub.execute_input":"2025-01-03T05:34:57.692752Z","iopub.status.idle":"2025-01-03T05:34:59.675365Z","shell.execute_reply.started":"2025-01-03T05:34:57.692724Z","shell.execute_reply":"2025-01-03T05:34:59.674043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shear_matrix = np.array([[1, 0.5, 0], [0, 1, 0], [0, 0, 1]])\n\n# Apply shearing using the transformation matrix\nshear_image = image_rgb.transform(\n    image_rgb.size, \n    Image.AFFINE, \n    data=(shear_matrix[0, 0], shear_matrix[0, 1], shear_matrix[0, 2],\n          shear_matrix[1, 0], shear_matrix[1, 1], shear_matrix[1, 2]),\n    resample=Image.BICUBIC)\n\n# Define the affine transformation matrix for stretching (stretch in x and y direction)\n# Stretch factors sx = 2, sy = 1.5\nstretch_matrix = np.array([[2, 0, 0], [0, 1.5, 0], [0, 0, 1]])\n\n# Apply stretching using the transformation matrix\nstretch_image = image_rgb.transform(\n    image_rgb.size, \n    Image.AFFINE, \n    data=(stretch_matrix[0, 0], stretch_matrix[0, 1], stretch_matrix[0, 2],\n          stretch_matrix[1, 0], stretch_matrix[1, 1], stretch_matrix[1, 2]),\n    resample=Image.BICUBIC)\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\naxs[0].imshow(shear_image)\naxs[0].set_title('Sheared Image (sx = 0.5)')\naxs[0].axis('off')\n\n# Display the stretched image\naxs[1].imshow(stretch_image)\naxs[1].set_title('Stretched Image (sx = 2, sy = 1.5)')\naxs[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:41:29.235867Z","iopub.execute_input":"2025-01-03T05:41:29.236269Z","iopub.status.idle":"2025-01-03T05:41:29.80214Z","shell.execute_reply.started":"2025-01-03T05:41:29.236234Z","shell.execute_reply":"2025-01-03T05:41:29.800931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"src_pts = np.float32([[100, 100], [400, 100], [100, 400], [400, 400]])\n\n# Define the destination points (4 points in the new perspective)\n# These points will represent where the source points should be mapped\ndst_pts = np.float32([[150, 150], [450, 100], [100, 450], [450, 450]])\n\n# Calculate the perspective transform matrix\nM = cv2.getPerspectiveTransform(src_pts, dst_pts)\n\n# Apply the perspective transformation to the image\nheight, width = image.shape[:2]\ntransformed_image = cv2.warpPerspective(image, M, (width, height))\n\n# Convert the images from BGR to RGB for displaying with matplotlib\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\ntransformed_image_rgb = cv2.cvtColor(transformed_image, cv2.COLOR_BGR2RGB)\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 4))\n\n# Display original image\naxs[0].imshow(image_rgb)\naxs[0].set_title('Original Image')\naxs[0].axis('off')\n\n# Display transformed image\naxs[1].imshow(transformed_image_rgb)\naxs[1].set_title('Perspective Transformed Image')\naxs[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:43:47.277373Z","iopub.execute_input":"2025-01-03T05:43:47.27776Z","iopub.status.idle":"2025-01-03T05:43:47.820168Z","shell.execute_reply.started":"2025-01-03T05:43:47.277727Z","shell.execute_reply":"2025-01-03T05:43:47.818883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_path = '/kaggle/input/doggyy/156318-812205657_small.mp4'  # Replace with your video path\ncap = cv2.VideoCapture(video_path)\n\n# Get video properties\nfps = cap.get(cv2.CAP_PROP_FPS)  # Frames per second\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Frame width\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Frame height\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Number of frames\n\n# Create a video writer to save the processed video\noutput_path = 'output_video.mp4'  # Path to save the processed video\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Initialize variables for computing the average of all frames\nframe_sum = np.zeros((frame_height, frame_width, 3), dtype=np.float32)\nframe_counter = 0\n\n# Iterate through all frames in the video\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    frame_counter += 1\n\n    # 1. Add text overlay\n    cv2.putText(frame, f\"Frame {frame_counter}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n\n    # 2. Perform rotation and flip\n    # Rotate the frame by 45 degrees\n    rows, cols, _ = frame.shape\n    M = cv2.getRotationMatrix2D((cols/2, rows/2), 45, 1)\n    rotated_frame = cv2.warpAffine(frame, M, (cols, rows))\n\n    # Flip the rotated frame horizontally\n    flipped_frame = cv2.flip(rotated_frame, 1)\n\n    # 3. Draw custom shapes (e.g., a rectangle and circle)\n    cv2.rectangle(flipped_frame, (50, 50), (300, 300), (0, 255, 0), 3)  # Rectangle\n    cv2.circle(flipped_frame, (500, 500), 50, (0, 0, 255), -1)  # Circle\n\n    # 4. Compute the average of all frames\n    frame_sum += flipped_frame\n    average_frame = frame_sum / frame_counter\n\n    # 5. Write the processed frame into the output video\n    out.write(flipped_frame)\n\n    # Display the processed frame\n    cv2.imshow('Processed Frame', flipped_frame)\n\n    # Press 'q' to stop the video processing early\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Final average frame (used for visualization or other purposes)\naverage_frame = np.uint8(average_frame)  # Convert to uint8 for display\ncv2.imshow('Average Frame', average_frame)\n\n# Release video objects\ncap.release()\nout.release()\n\n# Close all windows\ncv2.destroyAllWindows()\n\nprint(f\"Processed video saved at: {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:54:43.529884Z","iopub.execute_input":"2025-01-03T05:54:43.530311Z","execution_failed":"2025-01-03T05:54:44.694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}