{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7595263,"sourceType":"datasetVersion","datasetId":1004280}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction import text\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport nltk\nimport re\n#nltk.download('stopwords')\nstemmer = nltk.SnowballStemmer(\"english\")\nfrom nltk.corpus import stopwords\nimport string\nstopword=set(stopwords.words('english'))\n\nfrom tabulate import tabulate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-02T06:01:04.687716Z","iopub.execute_input":"2025-01-02T06:01:04.688091Z","iopub.status.idle":"2025-01-02T06:01:06.042229Z","shell.execute_reply.started":"2025-01-02T06:01:04.688059Z","shell.execute_reply":"2025-01-02T06:01:06.040976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import Data\ndata = pd.read_csv(\"/kaggle/input/book-recommendation-dataset/Books.csv\", nrows=67500)\ndata = data[[\"ISBN\", \"Book-Title\", \"Book-Author\", \"Year-Of-Publication\", \"Publisher\"]]\ndata.columns = [\"ISBN\", \"Title\", \"Author\", \"Year\", \"Publisher\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T06:01:06.043884Z","iopub.execute_input":"2025-01-02T06:01:06.044629Z","iopub.status.idle":"2025-01-02T06:01:06.911488Z","shell.execute_reply.started":"2025-01-02T06:01:06.044593Z","shell.execute_reply":"2025-01-02T06:01:06.910183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Cleaning\n# Remove Missing Value Data\nprint(data.isnull().sum())\ndata = data.dropna()\nprint(data.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T04:58:19.516026Z","iopub.execute_input":"2025-01-02T04:58:19.516413Z","iopub.status.idle":"2025-01-02T04:58:19.575279Z","shell.execute_reply.started":"2025-01-02T04:58:19.516386Z","shell.execute_reply":"2025-01-02T04:58:19.573341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Preprocessing\ndef clean(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    #text = [word for word in text.split(' ') if word not in stopword]\n    #text=\" \".join(text)\n    #text = [stemmer.stem(word) for word in text.split(' ')]\n    #text=\" \".join(text)\n    return text\ndata[\"cleaned_Title\"] = data[\"Title\"].apply(clean)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T04:58:22.689625Z","iopub.execute_input":"2025-01-02T04:58:22.690157Z","iopub.status.idle":"2025-01-02T04:58:23.605779Z","shell.execute_reply.started":"2025-01-02T04:58:22.690093Z","shell.execute_reply":"2025-01-02T04:58:23.604634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T04:58:26.304246Z","iopub.execute_input":"2025-01-02T04:58:26.304626Z","iopub.status.idle":"2025-01-02T04:58:26.329779Z","shell.execute_reply.started":"2025-01-02T04:58:26.304597Z","shell.execute_reply":"2025-01-02T04:58:26.328297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"indices = pd.Series(data.index,index=data['cleaned_Title']).drop_duplicates()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T04:58:28.755649Z","iopub.execute_input":"2025-01-02T04:58:28.756155Z","iopub.status.idle":"2025-01-02T04:58:28.768386Z","shell.execute_reply.started":"2025-01-02T04:58:28.756097Z","shell.execute_reply":"2025-01-02T04:58:28.766897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T04:58:30.755181Z","iopub.execute_input":"2025-01-02T04:58:30.755630Z","iopub.status.idle":"2025-01-02T04:58:30.815224Z","shell.execute_reply.started":"2025-01-02T04:58:30.755599Z","shell.execute_reply":"2025-01-02T04:58:30.813750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate Similarity\ndef get_similarity_cosine(title):\n    feature = data[\"cleaned_Title\"].tolist()\n    tfidf = text.TfidfVectorizer()\n    tfidf_matrix = tfidf.fit_transform(feature)\n    similarity_cosine = cosine_similarity(tfidf_matrix)\n    del tfidf\n    del tfidf_matrix\n    del feature\n    index = pd.Series(indices[title])\n    return similarity_cosine[index[0]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T04:58:34.794450Z","iopub.execute_input":"2025-01-02T04:58:34.794894Z","iopub.status.idle":"2025-01-02T04:58:34.801613Z","shell.execute_reply.started":"2025-01-02T04:58:34.794857Z","shell.execute_reply":"2025-01-02T04:58:34.799755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate Similarity\ndef similarity(ISBN, title, similarity_scores, reverse):\n    similarity_scores = list(enumerate(similarity_scores))\n    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=reverse)\n    similarity_scores = filter(lambda x: x[1] > 0, similarity_scores)\n    similarity_scores = list(similarity_scores)\n    movieindices = [i[0] for i in similarity_scores if i[0] < len(data)]\n    scores = [i[1] for i in similarity_scores if i[0] < len(data)]\n    result = pd.DataFrame([data.iloc[i] for i in movieindices])\n    result['Similarity'] = scores\n    result = result[['ISBN', 'Title', 'Similarity']]\n    result = result[result[\"ISBN\"] != ISBN]\n    result2 = result[result[\"Similarity\"] <= 0.5]\n    result = result[result[\"Similarity\"] > 0.5]\n    result = result[:20] if result.count()[0] >= 20 else result[:result.count()[0]]\n    result2 = result2[:20] if result2.count()[0] >= 20 else result2[:result2.count()[0]]\n    result = result.set_index([np.arange(1, result.count()[0] + 1)])\n    result2 = result2.set_index([np.arange(1, result2.count()[0] + 1)])\n    print(\"ISBN: \", ISBN)\n    print(\"Title: \", title)\n    print(\"Book Recommendation:\")\n    \n    print(tabulate(result, headers='keys', tablefmt = 'psql'))\n    print(tabulate(result2, headers='keys', tablefmt = 'psql'))\n        \n    del similarity_scores\n    del movieindices\n    del scores\n    del result\n    del result2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T04:58:44.198819Z","iopub.execute_input":"2025-01-02T04:58:44.199369Z","iopub.status.idle":"2025-01-02T04:58:44.214297Z","shell.execute_reply.started":"2025-01-02T04:58:44.199325Z","shell.execute_reply":"2025-01-02T04:58:44.212474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate Recommendation\ndef book_recommendation(ISBN, title):\n    clean_title = clean(title)\n    similarity_cosine = get_similarity_cosine(clean_title)\n    \n    similarity(ISBN, title, similarity_cosine, True)\n    \n    del similarity_cosine\n    del clean_title\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T04:58:47.913943Z","iopub.execute_input":"2025-01-02T04:58:47.914341Z","iopub.status.idle":"2025-01-02T04:58:47.920657Z","shell.execute_reply.started":"2025-01-02T04:58:47.914310Z","shell.execute_reply":"2025-01-02T04:58:47.918939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Book(ISBN):\n    book = data.loc[data[\"ISBN\"] == ISBN]\n    title = book.Title.item()\n    book_recommendation(ISBN, title)\n\nBook(\"051513290X\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T05:10:28.084104Z","iopub.execute_input":"2025-01-02T05:10:28.085834Z","iopub.status.idle":"2025-01-02T05:10:28.199399Z","shell.execute_reply.started":"2025-01-02T05:10:28.085707Z","shell.execute_reply":"2025-01-02T05:10:28.197224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, GRU\nimport numpy as np\n\n# Step 1: Prepare Data\ntexts = data['Book-Title'] + ' ' + data['Book-Author']  # Combine title and author for input\nlabels = data['Book-Title']  # Use book titles as the \"recommendation target\"\n\n# Tokenize the text\ntokenizer = Tokenizer(oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\npadded_sequences = pad_sequences(sequences, padding='post')\n\n# Encode labels\nlabel_tokenizer = Tokenizer()\nlabel_tokenizer.fit_on_texts(labels)\nlabel_sequences = label_tokenizer.texts_to_sequences(labels)\nlabel_padded = pad_sequences(label_sequences, padding='post')\n\n# Map indices to titles for decoding\nindex_to_title = {v: k for k, v in label_tokenizer.word_index.items()}\n\n# Step 2: Build RNN Model\nmodel = Sequential([\n    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=padded_sequences.shape[1]),\n    LSTM(128, return_sequences=False),\n    Dense(len(label_tokenizer.word_index) + 1, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Step 3: Train the Model\nlabels_for_training = np.array([x[0] if len(x) > 0 else 0 for x in label_padded])  # Handle empty sequences\nmodel.fit(padded_sequences, labels_for_training, epochs=10, batch_size=32)\n\n# Step 4: Make Recommendations\ndef recommend_book(book_title, num_recommendations=5):\n    # Preprocess input\n    seq = tokenizer.texts_to_sequences([book_title])\n    padded_seq = pad_sequences(seq, maxlen=padded_sequences.shape[1], padding='post')\n    \n    # Predict\n    predictions = model.predict(padded_seq)\n    top_indices = np.argsort(predictions[0])[-num_recommendations:][::-1]\n    \n    # Decode predictions\n    recommendations = [index_to_title[idx] for idx in top_indices if idx in index_to_title]\n    return recommendations\n\n# Test Recommendation\ntest_book = \"Harry Potter and the Sorcerer's Stone\"  # Example input\nrecommended_books = recommend_book(test_book)\nprint(\"Recommendations for:\", test_book)\nprint(recommended_books)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T06:01:36.607294Z","iopub.execute_input":"2025-01-02T06:01:36.607709Z","iopub.status.idle":"2025-01-02T06:01:36.703189Z","shell.execute_reply.started":"2025-01-02T06:01:36.607677Z","shell.execute_reply":"2025-01-02T06:01:36.701663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}